# InternVideo \[[è®ºæ–‡\]](https://arxiv.org/pdf/2212.03191.pdf)

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/action-classification-on-kinetics-400)](https://paperswithcode.com/sota/action-classification-on-kinetics-400?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/action-classification-on-kinetics-600)](https://paperswithcode.com/sota/action-classification-on-kinetics-600?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/action-classification-on-kinetics-700)](https://paperswithcode.com/sota/action-classification-on-kinetics-700?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/action-recognition-in-videos-on-something-1)](https://paperswithcode.com/sota/action-recognition-in-videos-on-something-1?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/action-recognition-in-videos-on-something)](https://paperswithcode.com/sota/action-recognition-in-videos-on-something?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/spatio-temporal-action-localization-on-ava)](https://paperswithcode.com/sota/spatio-temporal-action-localization-on-ava?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/action-recognition-on-ava-v2-2)](https://paperswithcode.com/sota/action-recognition-on-ava-v2-2?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/video-retrieval-on-activitynet)](https://paperswithcode.com/sota/video-retrieval-on-activitynet?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/video-retrieval-on-didemo)](https://paperswithcode.com/sota/video-retrieval-on-didemo?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/video-retrieval-on-msr-vtt)](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/video-retrieval-on-lsmdc)](https://paperswithcode.com/sota/video-retrieval-on-lsmdc?p=internvideo-general-video-foundation-models) 
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/video-retrieval-on-msvd)](https://paperswithcode.com/sota/video-retrieval-on-msvd?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/video-retrieval-on-vatex)](https://paperswithcode.com/sota/video-retrieval-on-vatex?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/zero-shot-video-retrieval-on-activitynet)](https://paperswithcode.com/sota/zero-shot-video-retrieval-on-activitynet?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/zero-shot-video-retrieval-on-didemo)](https://paperswithcode.com/sota/zero-shot-video-retrieval-on-didemo?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/zero-shot-video-retrieval-on-msr-vtt)](https://paperswithcode.com/sota/zero-shot-video-retrieval-on-msr-vtt?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/zero-shot-video-retrieval-on-lsmdc)](https://paperswithcode.com/sota/zero-shot-video-retrieval-on-lsmdc?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/zero-shot-video-retrieval-on-msvd)](https://paperswithcode.com/sota/zero-shot-video-retrieval-on-msvd?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/zero-shot-video-retrieval-on-vatex)](https://paperswithcode.com/sota/zero-shot-video-retrieval-on-vatex?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/visual-question-answering-on-msrvtt-qa-1)](https://paperswithcode.com/sota/visual-question-answering-on-msrvtt-qa-1?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/visual-question-answering-on-msvd-qa-1)](https://paperswithcode.com/sota/visual-question-answering-on-msvd-qa-1?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/visual-question-answering-on-tgif-qa)](https://paperswithcode.com/sota/visual-question-answering-on-tgif-qa?p=internvideo-general-video-foundation-models) 
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/open-set-action-recognition-on-ucf101-mitv2)](https://paperswithcode.com/sota/open-set-action-recognition-on-ucf101-mitv2?p=internvideo-general-video-foundation-models)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvideo-general-video-foundation-models/open-set-action-recognition-on-ucf-hmdb)](https://paperswithcode.com/sota/open-set-action-recognition-on-ucf-hmdb?p=internvideo-general-video-foundation-models)

åœ¨è¿™ä¸ªä»£ç ä»“åº“ä¸­ï¼Œæˆ‘ä»¬ç»™å‡ºInternVideoçš„å®˜æ–¹å®ç°ï¼Œ'[InternVideo: General Video Foundation Models via Generative and Discriminative Learning](https://arxiv.org/abs/2212.03191)'ã€‚

- **åœ¨Kinetics 400æ•°æ®é›†ä¸Šè·å–`91.1%`top-1å‡†ç¡®ç‡, `é¦–æ¬¡`çªç ´`90%`é‡Œç¨‹ç¢‘ã€‚**
- **åœ¨Something-Something V2æ•°æ®é›†ä¸Šè·å–`77.2%`top-1å‡†ç¡®ç‡ã€‚**
- **åœ¨`39`ä¸ªè§†é¢‘æ•°æ®é›†ï¼ˆåŒ…æ‹¬åŠ¨ä½œè¯†åˆ«ï¼Œæ—¶åºå®šä½ï¼Œæ£€ç´¢ç­‰ï¼‰ä¸Šè·å–`ä¸–ç•Œé¢†å…ˆ`æ€§èƒ½ï¼ˆäº2022å¹´å‘å¸ƒæ—¶ï¼‰ã€‚**

## æ›´æ–°
- `2023å¹´ 5æœˆ11æ—¥`: **è§†é¢‘æŒ‡ä»¤å¾®è°ƒæ•°æ®**å‘å¸ƒäº[è¿™å„¿](Data/instruction_data)ï¼Œå¯ç”¨äºå¾®è°ƒç«¯åˆ°ç«¯çš„è§†é¢‘å¯¹è¯ç³»ç»Ÿï¼Œæ¯”å¦‚[VideoChat](https://github.com/OpenGVLab/Ask-Anything)ã€‚
- `2023å¹´ 3æœˆ 8æ—¥`: æ‰€æœ‰é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹æƒé‡å·²ç»å‘å¸ƒã€‚è¯·ä»[è¿™é‡Œ](#model-zoo)æŸ¥çœ‹ã€‚
- `2023å¹´12æœˆ 6æ—¥`: InternVideoæŠ€æœ¯æŠ¥å‘Šå‘å¸ƒã€‚
- `2023å¹´ 9æœˆ 2æ—¥`: åª’ä½“å‘å¸ƒ ([å®˜æ–¹](https://www.shlab.org.cn/news/5443279) | [163æ–°é—»](https://www.163.com/dy/article/HG939TNR0530QRMB.html) | [qqæ–°é—»](https://new.qq.com/rain/a/20220902A053JP00))ã€‚

## å¼•è¨€
*æˆ‘ä»¬å±•ç¤ºäº†é¦–ä¸ªåœ¨è§†é¢‘å’Œè§†é¢‘-æ–‡æœ¬ä»»åŠ¡ä¸Šå‡å–å¾—é«˜æ€§èƒ½çš„è§†é¢‘åŸºç¡€æ¨¡å‹ã€‚*

æœ€è¿‘ï¼ŒåŸºç¡€æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„è¯¸å¤šä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹ä»…å…³æ³¨å›¾åƒçº§åˆ«çš„é¢„è®­ç»ƒå’Œé€‚åº”ï¼Œè¿™å¯¹äºåŠ¨æ€ä¸”å¤æ‚çš„è§†é¢‘çº§ç†è§£ä»»åŠ¡æ¥è¯´æ˜¯ä¸å¤Ÿçš„ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨è§†é¢‘åŸºç¡€æ¨¡å‹*InternVideo*ï¼Œå®ƒåˆ©ç”¨ç”Ÿæˆå’Œåˆ¤åˆ«è‡ªç›‘ç£è§†é¢‘å­¦ä¹ çš„ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´ï¼ŒInternVideoæœ‰æ•ˆåœ°æ¢ç´¢äº†è’™ç‰ˆè§†é¢‘å»ºæ¨¡ä¸è§†é¢‘-è¯­è¨€å¯¹æ¯”å­¦ä¹ ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡ï¼Œå¹¶ä»¥å¯å­¦ä¹ çš„æ–¹å¼é€‰æ‹©æ€§åœ°åè°ƒè¿™ä¸¤ä¸ªäº’è¡¥æ¡†æ¶ä¸­çš„è§†é¢‘è¡¨ç¤ºï¼Œä»è€Œæå‡å„ç§è§†é¢‘åº”ç”¨çš„æ€§èƒ½ã€‚InternVideo åœ¨åŒ…æ‹¬è§†é¢‘åŠ¨ä½œè¯†åˆ«/æ£€æµ‹ã€è§†é¢‘-è¯­è¨€å¯¹é½ã€å¼€æ”¾å¼è§†é¢‘åº”ç”¨ç­‰ä¼—å¤šä»»åŠ¡çš„ 39 ä¸ªè§†é¢‘æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ Kinetics-400 å’Œ Something-Something V2 åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº† *91.1%* å’Œ *77.2%* çš„ top-1 å‡†ç¡®ç‡ã€‚

## ä»£ç  & æ¨¡å‹
- [ ] é¢„è®­ç»ƒ.
    - [x] [è§†é¢‘æ©ç å»ºæ¨¡](Pretrain/VideoMAE)ã€‚
    - [x] [è§†é¢‘è¯­è¨€å¯¹æ¯”å­¦ä¹ ](Pretrain/Multi-Modalities-Pretraining)ã€‚
    - [x] [ViT (æ¥è‡ªæ©ç å­¦ä¹ )](Pretrain/VideoMAE#finetune) and [UniformerV2 (æ¥è‡ªå¤šæ¨¡æ€å­¦ä¹ )](https://github.com/OpenGVLab/UniFormerV2/blob/main/INSTRUCTIONS.md#training)çš„ç›‘ç£å­¦ä¹ ã€‚
    - [ ] æ¨¡å‹äº¤äº’ã€‚
- [ ] ä¸‹æ¸¸ä»»åŠ¡.
    - [ ] åŠ¨ä½œè¯†åˆ«ã€‚
    - [x] [æ—¶åºå®šä½](Downstream/Temporal-Action-Localization)ã€‚
    - [x] [æ—¶ç©ºå®šä½](Downstream/Spatial-Temporal-Action-Localization)ã€‚
    - [x] [è§†é¢‘æ–‡æœ¬æ£€ç´¢](Downstream/Video-Text-Retrieval)ã€‚
    - [x] [è§†é¢‘é—®ç­”](Downstream/multi-modalities-downstream#video-question-answering)ã€‚
    - [x] [è§†è§‰-è¯­è¨€å¯¼èˆª](Downstream/Visual-Language-Navigation)ã€‚
    - [x] [åŠ¨ä½œå¼€é›†è¯†åˆ«](Downstream/Open-Set-Action-Recognition)ã€‚
    - [x] [åŠ¨ä½œé›¶æ ·æœ¬è¯†åˆ«](Downstream/multi-modalities-downstream#zero-shot-action-recognition)ã€‚
    - [x] [é›¶æ ·æœ¬å¤šé¡¹é€‰æ‹©](Downstream/multi-modalities-downstream#zero-shot-multiple-choice)ã€‚
    - [x] [Ego4Dç›¸å…³å·¥ä½œ](https://github.com/OpenGVLab/ego4d-eccv2022-solutions)ã€‚
- [x] [é¢„è®­ç»ƒæ¨¡å‹æƒé‡](https://github.com/OpenGVLab/InternVideo#model-zoo)ã€‚
- [ ] è®­ç»ƒå’Œæµ‹è¯•çš„å±•ç¤ºã€‚

## æ€§èƒ½
- [è§†é¢‘æ£€ç´¢](Downstream/Video-Text-Retrieval#our-results)

## æ¨¡å‹åº“

<details>
<summary> é¢„è®­ç»ƒæ¨¡å‹ </summary>
<br>
<div>

|      æ¨¡å‹      |   è®­ç»ƒæ•°æ®   |                                               ä¸‹è½½                                                |
| :-----------------: | :----------------------: | :---------------------------------------------------------------------------------------------------: |
| InternVideo-MM-L-14 | WebVid10M+Self-collected (14M) |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/InternVideo-MM-L-14.ckpt) |
| VideoMAE-B | UnlabeledHybrid (1M) |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/videomae/vit_b_hybrid_pt_800e.pth)   |
| VideoMAE-L | UnlabeledHybrid (1M)|   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/videomae/vit_l_hybrid_pt_800e.pth)   |
| VideoMAE-H | UnlabeledHybrid (1M)|   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/videomae/vit_h_hybrid_pt_1200e.pth)  |
</div>
</details>

<details>
<summary> ä¸‹æ¸¸ä»»åŠ¡ </summary>
<br>
<div>

**åˆ†ç±»**
|      æ¨¡å‹      |   å¾®è°ƒæ•°æ®   |                                               ä¸‹è½½                                                |
| :-----------------: | :----------------: | :---------------------------------------------------------------------------------------------------: |
| VideoMAE-B | K400 |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/videomae/vit_b_hybrid_pt_800e_k400_ft.pth) |
| VideoMAE-B | K710 |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/videomae/vit_b_hybrid_pt_800e_k710_ft.pth)   |
| VideoMAE-B | SSv2 |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/videomae/vit_b_hybrid_pt_800e_ssv2_ft.pth)   |
| VideoMAE-L | K400 |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/videomae/vit_l_hybrid_pt_800e_k400_ft.pth) |
| VideoMAE-L | K700 |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/videomae/vit_l_hybrid_pt_800e_k700_ft.pth)   |
| VideoMAE-L | SSv2 |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/pretrain/videomae/vit_l_hybrid_pt_800e_ssv2_ft.pth)   |
| VideoMAE-H | K400 |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/action_recognition/K400/VideoMAE/ViT-H_f32_res384_89.54.pth) [log](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/action_recognition/K400/VideoMAE/ViT-H_f32_res384_89.54.log)|
| VideoMAE-H | SSv1 |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/action_recognition/SSV1/VideoMAE/ViT-H.pth) [log](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/action_recognition/SSV1/VideoMAE/log.txt)|
| VideoMAE-H | HMDB51 |   [ckpt_split1](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/action_recognition/HMDB51/VideoMAE/split1_89.64/ViT-H.pth) | [ckpt_split2](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/action_recognition/HMDB51/VideoMAE/split2_89.92/ViT-H.pth) | [ckpt_split3](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/action_recognition/HMDB51/VideoMAE/split3_88.35/ViT-H.pth)|

**æ£€ç´¢**
|      æ¨¡å‹      |   è®­ç»ƒæ•°æ®   |                                               ä¸‹è½½                                                |
| :-----------------: | :----------------: | :---------------------------------------------------------------------------------------------------: |
| InternVideo-MM-L-14 | ActivityNet |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/activitynet/kc4_1e-3_2e-3_bs64_77words_64frame_dsl/pytorch_model.bin) [opt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/activitynet/kc4_1e-3_2e-3_bs64_77words_64frame_dsl/pytorch_opt.bin) [log](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/activitynet/kc4_1e-3_2e-3_bs64_77words_64frame_dsl/log.txt)|
| InternVideo-MM-L-14 | DiDeMo |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/didemo/pytorch_model.bin) [opt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/didemo/pytorch_opt.bin) [log](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/didemo/log.txt)|
| InternVideo-MM-L-14 | LSMDC |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/lsmdc/pytorch_model.bin) [opt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/lsmdc/pytorch_opt.bin) [log](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/lsmdc/log.txt)|
| InternVideo-MM-L-14 | MSR-VTT |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/msrvtt/kc4_finetune_1e-32e-3_77words_12frames_128_16_bothdsl/pytorch_model.bin) [opt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/msrvtt/kc4_finetune_1e-32e-3_77words_12frames_128_16_bothdsl/pytorch_opt.bin) [log](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/msrvtt/kc4_finetune_1e-32e-3_77words_12frames_128_16_bothdsl/log.txt)|
| InternVideo-MM-L-14 | MSVD |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/msvd/pytorch_model.bin) [opt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/msvd/pytorch_opt.bin) [log](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/msvd/log.txt)|
| InternVideo-MM-L-14 | VATEX |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/vatex/pytorch_model.bin) [opt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/vatex/pytorch_opt.bin) [log](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/retrieval/vatex/log.txt)|

**è§†é¢‘é—®ç­”**
|      æ¨¡å‹      |   å¾®è°ƒæ•°æ®   |                                               ä¸‹è½½                                                |
| :-----------------: | :----------------: | :---------------------------------------------------------------------------------------------------: |
| InternVideo-MM-L-14 | MSR-VTT |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/vqa/msrvtt.ckpt) |
| InternVideo-MM-L-14 | MSVD |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/vqa/msvd.ckpt)   |
| InternVideo-MM-L-14 | TGIFQA |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/vqa/tqifqa.ckpt)   |

**æ—¶ç©ºå®šä½**
|      æ¨¡å‹      |   å¾®è°ƒæ•°æ®   |                                               ä¸‹è½½                                                |
| :-----------------: | :----------------: | :---------------------------------------------------------------------------------------------------: |
| VideoMAE-H | AVA-Kinetics |   [ckpt](https://pjlab-gvm-data.oss-cn-shanghai.aliyuncs.com/internvideo/stal/vit_h_hybrid_pt_k710_ft_ak_ft.sh) |
</div>
</details>

ä¸ºäº†è¿›ä¸€æ­¥æå‡æˆ‘ä»¬çš„å·¥ä½œæˆ–è€…ä¾¿äºæˆ‘ä»¬äº¤æµ, å¦‚æœæœ‰ç©ºè¯·å¡«å†™[é—®å·](https://wenjuan.feishu.cn/m?t=syQjww7QWNJi-jk5u) (æˆ–è€…æ‰«æä¸‹é¢çš„QRç ).

<img src="Media/download.png" width="200" height="260" alt="survey_icon"/>
<!--![survey_icon](Media/download.png){:height="50%" width="50%"}-->

<!--
## ğŸš€ğŸš€ Pretraining

We present the code of video masked modeling ([VideoMAE](Pretrain/VideoMAE/README.md)), video-language contrastive learning modeling (to be updated), and model interaction (to be updated). Partial supervised video post-pretraining are given in both [VideoMAE](Pretrain/VideoMAE/README.md) and [UniformerV2](https://github.com/OpenGVLab/UniFormerV2/blob/d390105e588665af5029bfcceed5b9975d4b13bb/README.md).


## ğŸš¢ğŸš¢ Downstram Tasks

* The instruction of video-text retrieval is in the [Retrieval.md](Downstream/Video-Text-Retrieval/README.md)
* The instruction of temporal action localization is in the [TAL.md](Downstream/Temporal-Action-Localization/README.md)
* The instruction of open-set action recognition is in the [OAR.md](Downstream/Open-Set-Action-Recognition/README.md)
* The instruction of ego-tasks is in the [EGO.md](https://github.com/OpenGVLab/ego4d-eccv2022-solutions/blob/main/readme.md)
-->

<!--

## News

- `Nov 24, 2022`: ğŸš€ğŸš€ InternVideo .

## Coming soon
- [ ] 

## Introduction

**InternVideo**

## Main Results on Downstream Tasks

**Action Recognition**

**Temporal Action Localization**

**Spatio-Temporal Action Localization**


## Acknowledgment
-->

## å¼•ç”¨

å¦‚æœè¿™é¡¹å·¥ä½œå¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨InternVideoå’Œç›¸å…³å·¥ä½œã€‚

```
@article{wang2022internvideo,
  title={InternVideo: General Video Foundation Models via Generative and Discriminative Learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and Xing, Sen and Chen, Guo and Pan, Junting and Yu, Jiashuo and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}

@article{wang2023videomae,
  title={VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking},
  author={Wang, Limin and Huang, Bingkun and Zhao, Zhiyu and Tong, Zhan and He, Yinan and Wang, Yi and Wang, Yali and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16727},
  year={2023}
}

@article{li2022uniformerv2,
  title={UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2211.09552},
  year={2022}
}

@article{li2023unmasked,
  title={Unmasked Teacher: Towards Training-Efficient Video Foundation Models},
  author={Li, Kunchang and Wang, Yali and Li, Yizhuo and Wang, Yi and He, Yinan and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16058},
  year={2023}
}
```
