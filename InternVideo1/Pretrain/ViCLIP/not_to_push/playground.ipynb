{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remaining dirs or files: ['config.json', 'train.log']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-04T17:21:44 | vindlu: \u001b[0mLogging to: tmp/train.log\n",
      "\u001b[32m2023-04-04T17:21:44 | utils.config_utils: \u001b[0mconfig: {\n",
      "  data_dir: /mnt/petrelfs/share/videointern/annotations/\n",
      "  data_root: /mnt/petrelfs/share/videointern/annotations/videos_images\n",
      "  anno_root_pt: /mnt/petrelfs/share/videointern/annotations/anno_pretrain\n",
      "  anno_root_downstream: /mnt/petrelfs/share/videointern/annotations/anno_downstream\n",
      "  available_corpus: {\n",
      "      cc3m: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC']\n",
      "      cc12m: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc12m_train.json', 'pssd:s3://GCC/GCC12m']\n",
      "      sbu: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/sbu.json', 'pssd:s3://SBU/']\n",
      "      vg: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/vg.json', 'pssd:s3://VG_dataset/']\n",
      "      coco: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/coco.json', 'pssd:s3://coco_caption']\n",
      "      imagenet1k: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/imagenet1k_train.json', '/mnt/petrelfs/share/images/train']\n",
      "      webvid: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_train.json', 'pssd:s3://WebVid2M', 'video']\n",
      "      webvid_10m: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_10m_train_clean.json', 'pssd:s3://WebVid10M', 'video']\n",
      "      kinetics400: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/kinetics400_train.json', 'pssd:s3://k400', 'video']\n",
      "      kinetics710: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/kinetics710_train.json', '', 'video']\n",
      "      kinetics710_raw: ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/kinetics710_raw_train.json', '', 'only_video']\n",
      "      coco_vg: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/coco.json', 'pssd:s3://coco_caption'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/vg.json', 'pssd:s3://VG_dataset/']]\n",
      "      in1k_k710: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/imagenet1k_train.json', '/mnt/petrelfs/share/images/train'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/kinetics710_train.json', '', 'video']]\n",
      "      webvid_cc3m: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_train.json', 'pssd:s3://WebVid2M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC']]\n",
      "      webvid_cc3m_in1k_k710: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_train.json', 'pssd:s3://WebVid2M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/imagenet1k_train.json', '/mnt/petrelfs/share/images/train'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/kinetics710_train.json', '', 'video']]\n",
      "      webvid_cc3m_k710raw: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_train.json', 'pssd:s3://WebVid2M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/kinetics710_raw_train.json', '', 'only_video']]\n",
      "      webvid_14m: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_train.json', 'pssd:s3://WebVid2M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/coco.json', 'pssd:s3://coco_caption'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/vg.json', 'pssd:s3://VG_dataset/'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/sbu.json', 'pssd:s3://SBU/'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc12m_train.json', 'pssd:s3://GCC/GCC12m']]\n",
      "      webvid12m_14m: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_train.json', 'pssd:s3://WebVid2M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_10m_train_clean.json', 'pssd:s3://WebVid10M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/coco.json', 'pssd:s3://coco_caption'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/vg.json', 'pssd:s3://VG_dataset/'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/sbu.json', 'pssd:s3://SBU/'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc12m_train.json', 'pssd:s3://GCC/GCC12m']]\n",
      "      webvid10m_14m: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_10m_train_clean.json', 'pssd:s3://WebVid10M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/coco.json', 'pssd:s3://coco_caption'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/vg.json', 'pssd:s3://VG_dataset/'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/sbu.json', 'pssd:s3://SBU/'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc12m_train.json', 'pssd:s3://GCC/GCC12m']]\n",
      "      simple_17m: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_train.json', 'pssd:s3://WebVid2M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc12m_train.json', 'pssd:s3://GCC/GCC12m']]\n",
      "      simple_25m: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_10m_train_clean.json', 'pssd:s3://WebVid10M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc12m_train.json', 'pssd:s3://GCC/GCC12m']]\n",
      "      msrvtt_1k_test: ['/mnt/petrelfs/share/videointern/annotations/anno_downstream/msrvtt_test1k.json', 's3://video_pub/MSR-VTT/MSRVTT_Videos', 'video'] }\n",
      "  VisionEncoders: {\n",
      "      beit: {\n",
      "          name: beit_base\n",
      "          pretrained: microsoft/beit-base-patch16-224-pt22k-ft22k\n",
      "          d_model: 768 }\n",
      "      beit_large: {\n",
      "          name: beit_large\n",
      "          pretrained: microsoft/beit-large-patch16-224-pt22k-ft22k\n",
      "          d_model: 1024 } }\n",
      "  TextEncoders: {\n",
      "      bert: {\n",
      "          name: bert_base\n",
      "          pretrained: bert-base-uncased\n",
      "          config: configs/config_bert.json\n",
      "          d_model: 768\n",
      "          fusion_layer: 9 }\n",
      "      bert_large: {\n",
      "          name: bert_large\n",
      "          pretrained: bert-large-uncased\n",
      "          config: configs/config_bert_large.json\n",
      "          d_model: 1024\n",
      "          fusion_layer: 19 } }\n",
      "  train_corpus: webvid_cc3m\n",
      "  train_file: [['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/webvid_train.json', 'pssd:s3://WebVid2M', 'video'], ['/mnt/petrelfs/share/videointern/annotations/anno_pretrain/cc3m_train.json', 'pssd:s3://GCC']]\n",
      "  test_file: {\n",
      "      msrvtt_qa_val: ['/mnt/petrelfs/share/videointern/annotations/anno_downstream/msrvtt_qa_val.json', 'pssd:s3://MSR-VTT/MSRVTT_Videos', 'video'] }\n",
      "  test_types: ['msrvtt_qa_val']\n",
      "  answer_list: /mnt/petrelfs/share/videointern/annotations/anno_downstream/msrvtt_qa_answer_list.json\n",
      "  num_workers: 6\n",
      "  stop_key: None\n",
      "  eos: \n",
      "  max_q_len: 25\n",
      "  max_a_len: 5\n",
      "  num_frames: 4\n",
      "  num_frames_test: 4\n",
      "  batch_size: 128\n",
      "  batch_size_test: 128\n",
      "  max_txt_l: 32\n",
      "  inputs: {\n",
      "      image_res: 224\n",
      "      video_input: {\n",
      "          num_frames: 4\n",
      "          sample_type: rand\n",
      "          num_frames_test: 4\n",
      "          sample_type_test: middle\n",
      "          random_aug: False }\n",
      "      max_txt_l: {\n",
      "          image: 32\n",
      "          video: 32 }\n",
      "      batch_size: {\n",
      "          image: 128\n",
      "          video: 128 }\n",
      "      batch_size_test: {\n",
      "          image: 128\n",
      "          video: 128 } }\n",
      "  vision_enc: clip_vision_b16\n",
      "  text_enc: bert\n",
      "  t5_model_name: google/flan-t5-base\n",
      "  model: {\n",
      "      model_cls: VindLU_BLIP_T5\n",
      "      vision_encoder: {\n",
      "          name: clip_vision_b16\n",
      "          d_model: 768\n",
      "          clip_return_layer: 1\n",
      "          clip_return_interval: 1\n",
      "          checkpoint_num: 0 }\n",
      "      text_encoder: {\n",
      "          name: bert_base\n",
      "          pretrained: bert-base-uncased\n",
      "          config: configs/config_bert.json\n",
      "          d_model: 768\n",
      "          fusion_layer: 9 }\n",
      "      t5_model_name: google/flan-t5-xl\n",
      "      requires_raw_text: True\n",
      "      vit_add_ln: True\n",
      "      embed_dim: 256\n",
      "      temp: 0.07\n",
      "      qformer_num_query_tokens: 32 }\n",
      "  criterion: {\n",
      "      loss_weight: {\n",
      "          pre: 1.0 }\n",
      "      vtm_hard_neg: False }\n",
      "  optimizer: {\n",
      "      opt: adamW\n",
      "      lr: 0.0001\n",
      "      opt_betas: [0.9, 0.999]\n",
      "      weight_decay: 0.02\n",
      "      max_grad_norm: -1\n",
      "      different_lr: {\n",
      "          enable: False\n",
      "          module_names: []\n",
      "          lr: 0.001 } }\n",
      "  scheduler: {\n",
      "      sched: cosine\n",
      "      epochs: 3\n",
      "      min_lr_multi: 0.01\n",
      "      warmup_epochs: 0.5 }\n",
      "  no_save_params_prefix: ['t5_model']\n",
      "  evaluate: True\n",
      "  deep_fusion: False\n",
      "  evaluation: {\n",
      "      eval_frame_ensemble: concat\n",
      "      eval_x_only: False\n",
      "      k_test: 128\n",
      "      eval_offload: True }\n",
      "  fp16: True\n",
      "  gradient_checkpointing: True\n",
      "  wandb: {\n",
      "      enable: False\n",
      "      entity: likunchang\n",
      "      project: lyz_vindlu_blip_t5 }\n",
      "  dist_url: env://\n",
      "  device: cuda\n",
      "  mode: pt\n",
      "  output_dir: tmp\n",
      "  resume: False\n",
      "  debug: False\n",
      "  log_freq: 100\n",
      "  seed: 42\n",
      "  pretrained_path: exp/exp_pretrain_blip/blip_T5_5m_16x128/blip_T5_xl_5m_16x128/ckpt_best.pth\n",
      "  rank: 0\n",
      "  gpu: 0\n",
      "  world_size: 1\n",
      "  distributed: True\n",
      "  dist_backend: nccl }\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"MASTER_ADDR\"] = \"SH-IDC1-10-140-37-139\"\n",
    "os.environ[\"MASTER_PORT\"] = \"33212\"\n",
    "sys.argv = [\n",
    "    'debug.py', \n",
    "    '/mnt/petrelfs/liyizhuo/projects/vindlu/exp/exp_pretrain_blip/blip_T5_5m_16x128/config.py', \n",
    "    'wandb.enable', \n",
    "    'False', \n",
    "    'output_dir', \n",
    "    'tmp',\n",
    "    'evaluate',\n",
    "    'True',\n",
    "    'model.t5_model_name', \n",
    "    'google/flan-t5-xl',\n",
    "    'pretrained_path',\n",
    "    'exp/exp_pretrain_blip/blip_T5_5m_16x128/blip_T5_xl_5m_16x128/ckpt_best.pth',\n",
    "    ]\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from models.vindlu_blip_T5 import VindLU_BLIP_T5\n",
    "from utils.config_utils import setup_main\n",
    "\n",
    "config = setup_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-04T17:21:50 | tasks.pretrain: \u001b[0mCreating dataset for pt\n",
      "\u001b[32m2023-04-04T17:21:50 | dataset.sqlite_dataset: \u001b[0mLoad json file\n",
      "\u001b[32m2023-04-04T17:22:11 | dataset.sqlite_dataset: \u001b[0mNum samples: 2881167\n",
      "\u001b[32m2023-04-04T17:22:11 | dataset.sqlite_dataset: \u001b[0mNum samples too short: 0\n",
      "\u001b[32m2023-04-04T17:22:11 | dataset.sqlite_dataset: \u001b[0mLoad json file\n",
      "\u001b[32m2023-04-04T17:22:34 | dataset.sqlite_dataset: \u001b[0mNum samples: 2487516\n",
      "\u001b[32m2023-04-04T17:22:34 | dataset.sqlite_dataset: \u001b[0mNum samples too short: 9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /mnt/petrelfs/share/videointern/annotations/anno_downstream/msrvtt_qa_val.json: 100%|██████████| 12278/12278 [00:00<00:00, 425963.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-04T17:22:35 | tasks.shared_utils: \u001b[0mCreating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-04T17:23:00 | models.vindlu_blip_qformer: \u001b[0mBuild vision_encoder: clip_vision_b16\n",
      "\u001b[32m2023-04-04T17:23:00 | models.backbones.vit.clip_vision: \u001b[0mReturn Layer: 1\n",
      "\u001b[32m2023-04-04T17:23:00 | models.backbones.vit.clip_vision: \u001b[0mReturn Interval: 1\n",
      "\u001b[32m2023-04-04T17:23:00 | models.backbones.vit.clip_vision: \u001b[0mWith LayerNorm and Proj: False\n",
      "\u001b[32m2023-04-04T17:23:01 | models.backbones.vit.clip_vision: \u001b[0mTeacher return index: [11]\n",
      "\u001b[32m2023-04-04T17:23:01 | models.backbones.vit.clip_vision: \u001b[0mLoad pretrained weights from /mnt/petrelfs/share_data/likunchang/model/clip_visual_encoder/vit_b16.pth\n",
      "\u001b[32m2023-04-04T17:23:03 | models.backbones.vit.clip_vision: \u001b[0mInflate: conv1.weight, torch.Size([768, 3, 16, 16]) => torch.Size([768, 3, 1, 16, 16])\n",
      "\u001b[32m2023-04-04T17:23:03 | models.backbones.vit.clip_vision: \u001b[0mInit center: True\n",
      "\u001b[32m2023-04-04T17:23:03 | models.backbones.vit.clip_vision: \u001b[0mUnexpected keys: ['proj', 'ln_post.weight', 'ln_post.bias']\n",
      "\u001b[32m2023-04-04T17:23:03 | models.backbones.vit.clip_vision: \u001b[0mMissing keys: []\n",
      "\u001b[32m2023-04-04T17:23:23 | models.vindlu_blip_T5: \u001b[0mLoading T5 model: google/flan-t5-xl...\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mdiff_names: [], diff_lr: None\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.vision_temp_embed: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.query_tokens: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.vision_layernorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.vision_layernorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.embeddings.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.embeddings.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.0.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.1.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.2.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.3.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.4.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.5.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.6.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.7.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.8.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.9.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.10.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.intermediate_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.intermediate_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.output_query.dense.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.output_query.dense.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.output_query.LayerNorm.weight: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.qformer.bert.encoder.layer.11.output_query.LayerNorm.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.t5_proj.weight: wd: 0.02, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0mparam module.t5_proj.bias: wd: 0, lr: 0.0001\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0moptimizer -- lr=0.0001 wd=0 len(p)=163\n",
      "\u001b[32m2023-04-04T17:25:31 | utils.optimizer: \u001b[0moptimizer -- lr=0.0001 wd=0.02 len(p)=97\n",
      "\u001b[32m2023-04-04T17:25:31 | tasks.shared_utils: \u001b[0mLoading checkpoint from exp/exp_pretrain_blip/blip_T5_5m_16x128/blip_T5_xl_5m_16x128/ckpt_best.pth\n",
      "\u001b[32m2023-04-04T17:25:41 | tasks.shared_utils: \u001b[0m_IncompatibleKeys(missing_keys=['t5_model.shared.weight', 't5_model.encoder.embed_tokens.weight', 't5_model.encoder.block.0.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.0.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.0.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.0.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 't5_model.encoder.block.0.layer.0.layer_norm.weight', 't5_model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.0.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.0.layer.1.layer_norm.weight', 't5_model.encoder.block.1.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.1.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.1.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.1.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.1.layer.0.layer_norm.weight', 't5_model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.1.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.1.layer.1.layer_norm.weight', 't5_model.encoder.block.2.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.2.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.2.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.2.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.2.layer.0.layer_norm.weight', 't5_model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.2.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.2.layer.1.layer_norm.weight', 't5_model.encoder.block.3.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.3.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.3.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.3.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.3.layer.0.layer_norm.weight', 't5_model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.3.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.3.layer.1.layer_norm.weight', 't5_model.encoder.block.4.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.4.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.4.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.4.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.4.layer.0.layer_norm.weight', 't5_model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.4.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.4.layer.1.layer_norm.weight', 't5_model.encoder.block.5.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.5.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.5.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.5.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.5.layer.0.layer_norm.weight', 't5_model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.5.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.5.layer.1.layer_norm.weight', 't5_model.encoder.block.6.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.6.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.6.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.6.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.6.layer.0.layer_norm.weight', 't5_model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.6.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.6.layer.1.layer_norm.weight', 't5_model.encoder.block.7.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.7.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.7.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.7.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.7.layer.0.layer_norm.weight', 't5_model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.7.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.7.layer.1.layer_norm.weight', 't5_model.encoder.block.8.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.8.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.8.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.8.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.8.layer.0.layer_norm.weight', 't5_model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.8.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.8.layer.1.layer_norm.weight', 't5_model.encoder.block.9.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.9.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.9.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.9.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.9.layer.0.layer_norm.weight', 't5_model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.9.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.9.layer.1.layer_norm.weight', 't5_model.encoder.block.10.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.10.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.10.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.10.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.10.layer.0.layer_norm.weight', 't5_model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.10.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.10.layer.1.layer_norm.weight', 't5_model.encoder.block.11.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.11.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.11.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.11.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.11.layer.0.layer_norm.weight', 't5_model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.11.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.11.layer.1.layer_norm.weight', 't5_model.encoder.block.12.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.12.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.12.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.12.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.12.layer.0.layer_norm.weight', 't5_model.encoder.block.12.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.12.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.12.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.12.layer.1.layer_norm.weight', 't5_model.encoder.block.13.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.13.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.13.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.13.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.13.layer.0.layer_norm.weight', 't5_model.encoder.block.13.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.13.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.13.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.13.layer.1.layer_norm.weight', 't5_model.encoder.block.14.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.14.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.14.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.14.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.14.layer.0.layer_norm.weight', 't5_model.encoder.block.14.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.14.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.14.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.14.layer.1.layer_norm.weight', 't5_model.encoder.block.15.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.15.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.15.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.15.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.15.layer.0.layer_norm.weight', 't5_model.encoder.block.15.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.15.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.15.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.15.layer.1.layer_norm.weight', 't5_model.encoder.block.16.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.16.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.16.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.16.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.16.layer.0.layer_norm.weight', 't5_model.encoder.block.16.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.16.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.16.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.16.layer.1.layer_norm.weight', 't5_model.encoder.block.17.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.17.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.17.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.17.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.17.layer.0.layer_norm.weight', 't5_model.encoder.block.17.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.17.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.17.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.17.layer.1.layer_norm.weight', 't5_model.encoder.block.18.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.18.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.18.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.18.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.18.layer.0.layer_norm.weight', 't5_model.encoder.block.18.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.18.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.18.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.18.layer.1.layer_norm.weight', 't5_model.encoder.block.19.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.19.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.19.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.19.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.19.layer.0.layer_norm.weight', 't5_model.encoder.block.19.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.19.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.19.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.19.layer.1.layer_norm.weight', 't5_model.encoder.block.20.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.20.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.20.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.20.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.20.layer.0.layer_norm.weight', 't5_model.encoder.block.20.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.20.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.20.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.20.layer.1.layer_norm.weight', 't5_model.encoder.block.21.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.21.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.21.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.21.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.21.layer.0.layer_norm.weight', 't5_model.encoder.block.21.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.21.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.21.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.21.layer.1.layer_norm.weight', 't5_model.encoder.block.22.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.22.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.22.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.22.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.22.layer.0.layer_norm.weight', 't5_model.encoder.block.22.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.22.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.22.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.22.layer.1.layer_norm.weight', 't5_model.encoder.block.23.layer.0.SelfAttention.q.weight', 't5_model.encoder.block.23.layer.0.SelfAttention.k.weight', 't5_model.encoder.block.23.layer.0.SelfAttention.v.weight', 't5_model.encoder.block.23.layer.0.SelfAttention.o.weight', 't5_model.encoder.block.23.layer.0.layer_norm.weight', 't5_model.encoder.block.23.layer.1.DenseReluDense.wi_0.weight', 't5_model.encoder.block.23.layer.1.DenseReluDense.wi_1.weight', 't5_model.encoder.block.23.layer.1.DenseReluDense.wo.weight', 't5_model.encoder.block.23.layer.1.layer_norm.weight', 't5_model.encoder.final_layer_norm.weight', 't5_model.decoder.embed_tokens.weight', 't5_model.decoder.block.0.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.0.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.0.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.0.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 't5_model.decoder.block.0.layer.0.layer_norm.weight', 't5_model.decoder.block.0.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.0.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.0.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.0.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.0.layer.1.layer_norm.weight', 't5_model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.0.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.0.layer.2.layer_norm.weight', 't5_model.decoder.block.1.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.1.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.1.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.1.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.1.layer.0.layer_norm.weight', 't5_model.decoder.block.1.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.1.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.1.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.1.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.1.layer.1.layer_norm.weight', 't5_model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.1.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.1.layer.2.layer_norm.weight', 't5_model.decoder.block.2.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.2.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.2.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.2.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.2.layer.0.layer_norm.weight', 't5_model.decoder.block.2.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.2.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.2.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.2.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.2.layer.1.layer_norm.weight', 't5_model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.2.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.2.layer.2.layer_norm.weight', 't5_model.decoder.block.3.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.3.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.3.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.3.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.3.layer.0.layer_norm.weight', 't5_model.decoder.block.3.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.3.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.3.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.3.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.3.layer.1.layer_norm.weight', 't5_model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.3.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.3.layer.2.layer_norm.weight', 't5_model.decoder.block.4.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.4.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.4.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.4.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.4.layer.0.layer_norm.weight', 't5_model.decoder.block.4.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.4.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.4.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.4.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.4.layer.1.layer_norm.weight', 't5_model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.4.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.4.layer.2.layer_norm.weight', 't5_model.decoder.block.5.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.5.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.5.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.5.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.5.layer.0.layer_norm.weight', 't5_model.decoder.block.5.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.5.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.5.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.5.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.5.layer.1.layer_norm.weight', 't5_model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.5.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.5.layer.2.layer_norm.weight', 't5_model.decoder.block.6.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.6.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.6.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.6.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.6.layer.0.layer_norm.weight', 't5_model.decoder.block.6.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.6.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.6.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.6.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.6.layer.1.layer_norm.weight', 't5_model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.6.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.6.layer.2.layer_norm.weight', 't5_model.decoder.block.7.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.7.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.7.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.7.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.7.layer.0.layer_norm.weight', 't5_model.decoder.block.7.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.7.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.7.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.7.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.7.layer.1.layer_norm.weight', 't5_model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.7.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.7.layer.2.layer_norm.weight', 't5_model.decoder.block.8.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.8.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.8.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.8.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.8.layer.0.layer_norm.weight', 't5_model.decoder.block.8.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.8.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.8.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.8.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.8.layer.1.layer_norm.weight', 't5_model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.8.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.8.layer.2.layer_norm.weight', 't5_model.decoder.block.9.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.9.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.9.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.9.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.9.layer.0.layer_norm.weight', 't5_model.decoder.block.9.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.9.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.9.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.9.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.9.layer.1.layer_norm.weight', 't5_model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.9.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.9.layer.2.layer_norm.weight', 't5_model.decoder.block.10.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.10.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.10.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.10.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.10.layer.0.layer_norm.weight', 't5_model.decoder.block.10.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.10.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.10.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.10.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.10.layer.1.layer_norm.weight', 't5_model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.10.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.10.layer.2.layer_norm.weight', 't5_model.decoder.block.11.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.11.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.11.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.11.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.11.layer.0.layer_norm.weight', 't5_model.decoder.block.11.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.11.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.11.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.11.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.11.layer.1.layer_norm.weight', 't5_model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.11.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.11.layer.2.layer_norm.weight', 't5_model.decoder.block.12.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.12.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.12.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.12.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.12.layer.0.layer_norm.weight', 't5_model.decoder.block.12.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.12.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.12.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.12.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.12.layer.1.layer_norm.weight', 't5_model.decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.12.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.12.layer.2.layer_norm.weight', 't5_model.decoder.block.13.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.13.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.13.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.13.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.13.layer.0.layer_norm.weight', 't5_model.decoder.block.13.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.13.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.13.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.13.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.13.layer.1.layer_norm.weight', 't5_model.decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.13.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.13.layer.2.layer_norm.weight', 't5_model.decoder.block.14.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.14.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.14.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.14.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.14.layer.0.layer_norm.weight', 't5_model.decoder.block.14.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.14.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.14.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.14.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.14.layer.1.layer_norm.weight', 't5_model.decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.14.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.14.layer.2.layer_norm.weight', 't5_model.decoder.block.15.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.15.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.15.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.15.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.15.layer.0.layer_norm.weight', 't5_model.decoder.block.15.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.15.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.15.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.15.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.15.layer.1.layer_norm.weight', 't5_model.decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.15.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.15.layer.2.layer_norm.weight', 't5_model.decoder.block.16.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.16.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.16.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.16.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.16.layer.0.layer_norm.weight', 't5_model.decoder.block.16.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.16.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.16.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.16.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.16.layer.1.layer_norm.weight', 't5_model.decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.16.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.16.layer.2.layer_norm.weight', 't5_model.decoder.block.17.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.17.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.17.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.17.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.17.layer.0.layer_norm.weight', 't5_model.decoder.block.17.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.17.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.17.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.17.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.17.layer.1.layer_norm.weight', 't5_model.decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.17.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.17.layer.2.layer_norm.weight', 't5_model.decoder.block.18.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.18.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.18.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.18.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.18.layer.0.layer_norm.weight', 't5_model.decoder.block.18.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.18.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.18.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.18.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.18.layer.1.layer_norm.weight', 't5_model.decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.18.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.18.layer.2.layer_norm.weight', 't5_model.decoder.block.19.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.19.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.19.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.19.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.19.layer.0.layer_norm.weight', 't5_model.decoder.block.19.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.19.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.19.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.19.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.19.layer.1.layer_norm.weight', 't5_model.decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.19.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.19.layer.2.layer_norm.weight', 't5_model.decoder.block.20.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.20.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.20.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.20.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.20.layer.0.layer_norm.weight', 't5_model.decoder.block.20.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.20.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.20.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.20.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.20.layer.1.layer_norm.weight', 't5_model.decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.20.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.20.layer.2.layer_norm.weight', 't5_model.decoder.block.21.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.21.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.21.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.21.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.21.layer.0.layer_norm.weight', 't5_model.decoder.block.21.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.21.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.21.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.21.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.21.layer.1.layer_norm.weight', 't5_model.decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.21.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.21.layer.2.layer_norm.weight', 't5_model.decoder.block.22.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.22.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.22.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.22.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.22.layer.0.layer_norm.weight', 't5_model.decoder.block.22.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.22.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.22.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.22.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.22.layer.1.layer_norm.weight', 't5_model.decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.22.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.22.layer.2.layer_norm.weight', 't5_model.decoder.block.23.layer.0.SelfAttention.q.weight', 't5_model.decoder.block.23.layer.0.SelfAttention.k.weight', 't5_model.decoder.block.23.layer.0.SelfAttention.v.weight', 't5_model.decoder.block.23.layer.0.SelfAttention.o.weight', 't5_model.decoder.block.23.layer.0.layer_norm.weight', 't5_model.decoder.block.23.layer.1.EncDecAttention.q.weight', 't5_model.decoder.block.23.layer.1.EncDecAttention.k.weight', 't5_model.decoder.block.23.layer.1.EncDecAttention.v.weight', 't5_model.decoder.block.23.layer.1.EncDecAttention.o.weight', 't5_model.decoder.block.23.layer.1.layer_norm.weight', 't5_model.decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 't5_model.decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 't5_model.decoder.block.23.layer.2.DenseReluDense.wo.weight', 't5_model.decoder.block.23.layer.2.layer_norm.weight', 't5_model.decoder.final_layer_norm.weight', 't5_model.lm_head.weight'], unexpected_keys=[])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-04T17:25:41 | tasks.shared_utils: \u001b[0mLoaded checkpoint from exp/exp_pretrain_blip/blip_T5_5m_16x128/blip_T5_xl_5m_16x128/ckpt_best.pth\n"
     ]
    }
   ],
   "source": [
    "from tasks.shared_utils import setup_model\n",
    "from tasks.pretrain import setup_dataloaders\n",
    "from utils.basic_utils import setup_seed\n",
    "\n",
    "is_pretrain = config.mode == \"pt\"\n",
    "\n",
    "setup_seed(42)\n",
    "device = torch.device(config.device)\n",
    "\n",
    "train_loaders, test_name2loaders, train_media_types = setup_dataloaders(\n",
    "    config, mode=config.mode\n",
    ")\n",
    "num_steps_per_epoch = sum(len(d) for d in train_loaders)\n",
    "config.scheduler.num_training_steps = num_steps_per_epoch * config.scheduler.epochs\n",
    "config.scheduler.num_warmup_steps = num_steps_per_epoch * config.scheduler.warmup_epochs\n",
    "\n",
    "model_cls = eval(config.model.get('model_cls', 'VindLU'))\n",
    "(\n",
    "    model,\n",
    "    model_without_ddp,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    scaler,\n",
    "    tokenizer,\n",
    "    start_epoch,\n",
    "    global_step,\n",
    ") = setup_model(\n",
    "config,\n",
    "model_cls=model_cls,\n",
    "has_decoder=False,\n",
    "pretrain=is_pretrain,\n",
    "find_unused_parameters=False if \"VindLU_BLIP\" in \\\n",
    "    config.model.get('model_cls', 'VindLU') else True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = VindLU_BLIP_T5(config, tokenizer=tokenizer)\n",
    "\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(test_name2loaders['msrvtt_qa_val'].dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "what are three people sitting on\n"
     ]
    }
   ],
   "source": [
    "image, text, idx = data\n",
    "\n",
    "print(image.shape)\n",
    "print(text)\n",
    "\n",
    "image = image.unsqueeze(0).cuda()\n",
    "text = [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full precision\n",
      "\u001b[32m2023-04-04T17:25:47 | models.vindlu_blip_T5: \u001b[0mraw_text_input: what are three raw_text_output: people sitting on generated_text: [0, 3567, 692, 16, 8] \n",
      "Float16\n",
      "\u001b[32m2023-04-04T17:25:47 | models.vindlu_blip_T5: \u001b[0mraw_text_input: what raw_text_output: are three people sitting on generated_text: [0, 0, 0, 0, 0] \n",
      "BFloat16\n",
      "\u001b[32m2023-04-04T17:25:48 | models.vindlu_blip_T5: \u001b[0mraw_text_input: what raw_text_output: are three people sitting on generated_text: [0, 3, 9, 248, 706] \n"
     ]
    }
   ],
   "source": [
    "print(\"Full precision\")\n",
    "model(image, None, idx=None, raw_text=text, log_generation=True)\n",
    "print(\"Float16\")\n",
    "with torch.cuda.amp.autocast(enabled=config.fp16, dtype=torch.float16):\n",
    "    model(image, None, idx=None, raw_text=text, log_generation=True)\n",
    "print(\"BFloat16\")\n",
    "with torch.cuda.amp.autocast(enabled=config.fp16, dtype=torch.bfloat16):\n",
    "    model(image, None, idx=None, raw_text=text, log_generation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "image = torch.zeros(1, 4, 3, 224, 224)\n",
    "image = image.cuda()\n",
    "max_input_len, max_output_len = 25, 5\n",
    "raw_question = raw_text_input = [\"What is in the picture?\"]\n",
    "raw_answer = raw_text_output = [\"A dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model.module\n",
    "\n",
    "_, input_t5 = self.encode_vision(image)\n",
    "input_t5 = self.t5_proj(input_t5)\n",
    "atts_t5 = torch.ones(input_t5.size()[:-1], dtype=torch.long).to(input_t5.device)\n",
    "\n",
    "input_tokens = self.tokenize(raw_text_input, input_t5.device, max_input_len)\n",
    "output_tokens = self.tokenize(\n",
    "    raw_text_output, input_t5.device, max_output_len\n",
    ")\n",
    "\n",
    "encoder_atts = torch.cat([atts_t5, input_tokens.attention_mask], dim=1)\n",
    "\n",
    "targets = output_tokens.input_ids.masked_fill(\n",
    "    output_tokens.input_ids == self.t5_tokenizer.pad_token_id, -100\n",
    ")\n",
    "\n",
    "inputs_embeds = self.t5_model.encoder.embed_tokens(input_tokens.input_ids)\n",
    "inputs_embeds = torch.cat([input_t5, inputs_embeds], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = self.generate(inputs_embeds, encoder_atts)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.t5_tokenizer.batch_decode(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
